{
  "qformer_hidden_size": 768,
  "blocks_num": 6,
  "num_heads": 12,
  "num_object_queries": 32,
  "num_relation_queries": 64,
  "num_global_queries": 32,
  "num_reasoning_hops": 4,
  "num_relation_types": 16,
  "num_epochs": 50,
  "batch_size": 48,
  "lr": 1.5e-4,
  "betas": [0.9, 0.999],
  "dropout_rate": 0.2,
  "weight_decay": 0.05,
  "eps": 1e-8,
  "sequence_size": 32,
  "unfreeze_layers": 0,
  "use_clip_for_text": true,
  "gradient_clip_norm": 1.0,
  "warmup_steps": 300,
  "patience": 10,
  "seed": 42,
  
  "num_workers": 8,
  "pin_memory": true,
  "precision": "bf16-mixed",
  
  "use_data_augmentation": true,
  "augmentation_prob": 0.7,

  "num_gpus": 2,
  "strategy": "ddp",
  
  "comments": {
    "batch_size": "48 per GPU -> effective batch size = 96 with 2 GPUs (reduced due to GNN+NSM)",
    "lr": "1.5e-4 - slightly higher due to larger effective batch size (linear scaling)",
    "num_reasoning_hops": "4 - number of NSM reasoning steps",
    "num_relation_types": "16 - SGG relation categories",
    "warmup_steps": "300 - adjusted for 2x throughput",
    "num_workers": "8 per process - total 16 workers for data loading",
    "strategy": "ddp - Distributed Data Parallel for multi-GPU training"
  }
}

